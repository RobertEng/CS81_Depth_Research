{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import cdflib\n",
    "from constants import HUMAN_ANNOTATION_PATH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(HUMAN_ANNOTATION_PATH) as f:\n",
    "    _human_dataset = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'c_id': 55011271,\n",
       " u'filename': u'human36m_train_0000521983.jpg',\n",
       " u'frame': 0,\n",
       " u'height': 256,\n",
       " u'id': 521983,\n",
       " u's_id': 11,\n",
       " u'video': u'Discussion 1.55011271.mp4',\n",
       " u'width': 256}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_human_dataset.keys()\n",
    "_human_dataset['images'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named imageio",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-90b763561aa6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m17\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#import skvideo.io\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mimageio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m  \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmisc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimresize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named imageio"
     ]
    }
   ],
   "source": [
    "## imports\n",
    "from os import listdir\n",
    "import json, sys\n",
    "import numpy as np\n",
    "import random\n",
    "random.seed(17)\n",
    "#import skvideo.io\n",
    "import imageio\n",
    "from  scipy.misc import imresize\n",
    "\n",
    "# from spacepy import pycdf\n",
    "import cdflib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "## set paths\n",
    "DATASET_DIR      = '../datasets/human36m_original'\n",
    "ANNOTATIONS_DIR  = '../datasets/human36m_annotations'\n",
    "IMAGES_DIR       = '../datasets/human36m_images'\n",
    "\n",
    "## dataset constants\n",
    "FEATURE_TYPES = ['D2_Positions','D3_Positions_mono']\n",
    "IMAGES_WIDTH  = 256\n",
    "IMAGES_HEIGHT = 256\n",
    "SKIP_FRAMES   = 20\n",
    "SKIP_ACTIONS  = ['_ALL']#,'WalkTogether','Posing','Photo','SittingDown','Directions','Purchases','Sitting','Walking','Waiting','Phoning','Smoking','WalkDog','Discussion','Eating']\n",
    "#CAMERA_IDS    = [54138969, 55011271, 58860488, 60457274]\n",
    "SUBJECT_IDS   = [1,5,6,7,8,9,11]\n",
    "HUMAN_36M_KEYPOINTS = \\\n",
    "  ['mid_hip',\n",
    "   'right_hip', 'right_knee', 'right_ankle', 'right_foot_base', 'right_foot_tip',\n",
    "   'left_hip', 'left_knee', 'left_ankle', 'left_foot_base', 'left_foot_tip',\n",
    "   'mid_hip_2', 'mid_spine', 'neck', 'chin', 'head', 'neck_2',\n",
    "   'left_shoulder', 'left_elbow', 'left_wrist', 'left_wrist_2','left_palm','left_thumb','left_thumb_2',\n",
    "   'neck_3',\n",
    "   'right_shoulder', 'right_elbow', 'right_wrist', 'right_wrist_2','right_palm','right_thumb','right_thumb_3']\n",
    "INTEREST_KEYPOINTS = \\\n",
    "  ['head', 'neck',\n",
    "   'left_shoulder', 'right_shoulder',\n",
    "   'left_elbow', 'right_elbow',\n",
    "   'left_wrist', 'right_wrist',\n",
    "   'left_hip', 'right_hip',\n",
    "   'left_knee', 'right_knee',\n",
    "   'left_ankle', 'right_ankle']\n",
    "SKELETON = \\\n",
    "  [['head', 'neck'],\n",
    "   ['neck', 'left_shoulder'], ['neck', 'right_shoulder'],\n",
    "   ['left_shoulder', 'left_elbow'], ['right_shoulder', 'right_elbow'],\n",
    "   ['left_elbow', 'left_wrist'], ['right_elbow', 'right_wrist'],\n",
    "   ['left_shoulder', 'left_hip'], ['right_shoulder', 'right_hip'],\n",
    "   ['left_hip', 'left_knee'], ['right_hip', 'right_knee'],\n",
    "   ['left_knee', 'left_ankle'], ['right_knee', 'right_ankle']]\n",
    "\n",
    "## create json annotations for the human3.6m dataset for the specified settings\n",
    "human36m = {}\n",
    "human36m['annotations'] = []\n",
    "human36m['images']      = []\n",
    "human36m['actions']     = []\n",
    "human36m['pose']        = []\n",
    "\n",
    "annotation = {}\n",
    "annotation['id']   = -1\n",
    "annotation['s_id'] = -1\n",
    "annotation['a_id'] = -1\n",
    "annotation['i_id'] = -1\n",
    "annotation['kpts_2d'] = []\n",
    "annotation['kpts_3d'] = []\n",
    "\n",
    "image = {}\n",
    "image['id']       = -1\n",
    "image['c_id']     = -1\n",
    "image['filename'] = ''\n",
    "image['width']    = IMAGES_WIDTH\n",
    "image['height']   = IMAGES_HEIGHT\n",
    "image['video']    = ''\n",
    "image['frame']    = -1\n",
    "\n",
    "actions = {}\n",
    "actions['id']       = -1\n",
    "actions['name']     = ''\n",
    "actions['version']  = ''\n",
    "\n",
    "pose = {}\n",
    "pose['keypoints']      = INTEREST_KEYPOINTS\n",
    "pose['skeleton']       = [map(INTEREST_KEYPOINTS.index,t) for t in SKELETON]\n",
    "pose['original_index'] = [HUMAN_36M_KEYPOINTS.index(k) for k in INTEREST_KEYPOINTS]\n",
    "human36m['pose'] = [pose]\n",
    "\n",
    "PADDED   = []\n",
    "USED_IDS = set()\n",
    "\n",
    "for subject_id in SUBJECT_IDS:\n",
    "    VIDEOS_DIR   = '%s/S%d/MyVideos'%(DATASET_DIR, subject_id)\n",
    "    FEATURES_DIR = '%s/S%d/MyPoseFeatures'%(DATASET_DIR, subject_id)\n",
    "\n",
    "    VIDEOS = listdir(VIDEOS_DIR)\n",
    "    for video_filename in VIDEOS:\n",
    "        video_info  = video_filename.split('.')\n",
    "        action_info = video_info[0].split(' ')\n",
    "\n",
    "        camera_id   = int(video_info[1])\n",
    "        action_name = action_info[0]\n",
    "        action_version = int(action_info[1]) if len(action_info) > 1 else 0\n",
    "\n",
    "        if action_name in SKIP_ACTIONS: continue\n",
    "        print \"S%d\"%subject_id, action_name, action_version, camera_id\n",
    "\n",
    "        # insert the action in the actions list if it has never been encountered\n",
    "        new_action = [a for a in human36m['actions'] if \\\n",
    "                        a['name'] == action_name and \\\n",
    "                        a['version'] == action_version]\n",
    "        if len(new_action)==0:\n",
    "            action_id = len(human36m['actions'])\n",
    "            action = {}\n",
    "            action['id']       = action_id\n",
    "            action['name']     = action_name\n",
    "            action['version']  = action_version\n",
    "            human36m['actions'].append(action)\n",
    "        else:\n",
    "            action_id = new_action[0]['id']\n",
    "\n",
    "        # extract the image frames\n",
    "        frames = imageio.get_reader(VIDEOS_DIR + '/' + video_filename,  'ffmpeg')\n",
    "        print \"Frames %d\"%(len(frames))\n",
    "\n",
    "        features = []\n",
    "        shapes   = []\n",
    "        # extract the features\n",
    "        for feature_type in FEATURE_TYPES:\n",
    "            cdf = cdflib.CDF(FEATURES_DIR + '/' + feature_type + '/' + '.'.join(video_info[:-1]) + '.cdf')\n",
    "            # cdf = pycdf.CDF(FEATURES_DIR + '/' + feature_type + '/' + '.'.join(video_info[:-1]) + '.cdf')\n",
    "            features.append(cdf['Pose'][0,:,:][...])\n",
    "\n",
    "            shape   = cdf['Pose'][0,:,:][...].shape\n",
    "            shapes.append(shape[0])\n",
    "            print feature_type, shape\n",
    "\n",
    "        assert(shapes.count(shapes[0]) == len(shapes))\n",
    "        print \"=================================\"\n",
    "\n",
    "        frame_num = 0\n",
    "        # access features and image one frame at the time\n",
    "        # NOTE: number of frames and number of features might be different!\n",
    "        # assumption is that they are alligned at beginning and the final\n",
    "        # frames get discarded.\n",
    "        while frame_num < shapes[0]:\n",
    "            frame   = frames.get_data(frame_num)\n",
    "\n",
    "            # 2d pose associated with that frame\n",
    "            pose_2d = features[FEATURE_TYPES.index('D2_Positions')][frame_num,:]\n",
    "            pose_2d_x = pose_2d[0::2]\n",
    "            pose_2d_y = pose_2d[1::2]\n",
    "\n",
    "            w  = max(pose_2d_x) - min(pose_2d_x)\n",
    "            h  = max(pose_2d_y) - min(pose_2d_y)\n",
    "            cx = int(min(pose_2d_x) + w/2.)\n",
    "            cy = int(min(pose_2d_y) + h/2.)\n",
    "\n",
    "            bbox = [cx - (w*1.2)/2., cy - (h*1.2)/2., w*1.2, h*1.2] # 20% enlarged\n",
    "            slack = int(bbox[2]/2.) if w > h else int(bbox[3]/2.)\n",
    "            x_start = cx - slack\n",
    "            x_end   = cx + slack\n",
    "            y_start = cy - slack\n",
    "            y_end   = cy + slack\n",
    "\n",
    "            # print cx, cy, w, h\n",
    "            # print bbox\n",
    "            # print slack\n",
    "            # print x_start, x_end\n",
    "            # print y_start, y_end\n",
    "\n",
    "            pad_left   = abs(x_start) if x_start < 0 else 0\n",
    "            #pad_right  = x_end - 2 * slack if x_end > 2 * slack else 0\n",
    "            pad_top    = abs(y_start) if y_start < 0 else 0\n",
    "            # pad_bottom = y_end - 2 * slack if y_end > 2 * slack else 0\n",
    "            padded_frame = np.pad(frame,((0,0),(pad_left,0),(0,0)),'edge')\n",
    "            # try:\n",
    "            crop = imresize(padded_frame[y_start+pad_top:y_end+pad_top, x_start+pad_left:x_end+pad_left, :],(IMAGES_WIDTH, IMAGES_WIDTH))\n",
    "            # except:\n",
    "            #     print frame.shape\n",
    "            #     print cx, x_start, x_end, 2*slack\n",
    "            #     print cy, y_start, y_end, 2*slack\n",
    "            #     print pad_top, pad_bottom, pad_left, pad_top\n",
    "            #     assert(False)\n",
    "\n",
    "            resize_ratio = [IMAGES_WIDTH / (2. * slack), IMAGES_HEIGHT / (2. * slack)]\n",
    "            keypoints_2d = []\n",
    "            for i in human36m['pose'][0]['original_index']:\n",
    "                pose_2d_x_i = (pose_2d_x[i] - x_start) * resize_ratio[0]\n",
    "                pose_2d_y_i = (pose_2d_y[i] - y_start) * resize_ratio[1]\n",
    "                keypoints_2d.extend([pose_2d_x_i,pose_2d_y_i])\n",
    "\n",
    "            # fig = plt.figure()\n",
    "            # ax = plt.subplot(121)\n",
    "            # ax.imshow(frame)\n",
    "            # ax.scatter(pose_2d_x,pose_2d_y,c='g',s=50)\n",
    "            # ax.scatter(cx,cy,c='r',s=50)\n",
    "            # rect  = patches.Rectangle((bbox[0],bbox[1]),bbox[2],bbox[3],linewidth=2,edgecolor='b',facecolor='none')\n",
    "            # ax.add_patch(rect)\n",
    "            # rect1  = patches.Rectangle((x_start,y_start),2*slack,2*slack,linewidth=2,edgecolor='y',facecolor='none')\n",
    "            # ax.add_patch(rect1)\n",
    "            # ax = plt.subplot(122)\n",
    "            # ax.imshow(crop)\n",
    "            # ax.scatter(keypoints_2d[0::2],keypoints_2d[1::2],c='g',s=50)\n",
    "            # plt.show()\n",
    "\n",
    "            # 3d pose associated with that frame\n",
    "            pose_3d = features[FEATURE_TYPES.index('D3_Positions_mono')][frame_num,:]\n",
    "            pose_3d_x = pose_3d[0::3]\n",
    "            pose_3d_y = pose_3d[1::3]\n",
    "            pose_3d_z = pose_3d[2::3]\n",
    "            keypoints_3d = []\n",
    "            for i in human36m['pose'][0]['original_index']:\n",
    "                keypoints_3d.extend([pose_3d_x[i],pose_3d_y[i],pose_3d_z[i]])\n",
    "            # fig = plt.figure()\n",
    "            # ax = fig.add_subplot(111, projection='3d')\n",
    "            # ax.scatter(keypoints_3d[0::3],keypoints_3d[1::3],keypoints_3d[2::3])\n",
    "            # plt.show()\n",
    "\n",
    "            rand_id       = random.randint(0,999999)\n",
    "            while rand_id in USED_IDS:\n",
    "                rand_id       = random.randint(0,999999)\n",
    "            USED_IDS.add(rand_id)\n",
    "\n",
    "            annotation_id = rand_id\n",
    "            image_id      = rand_id\n",
    "\n",
    "            annotation = {}\n",
    "            annotation['id']   = annotation_id\n",
    "            annotation['s_id'] = subject_id\n",
    "            annotation['a_id'] = action_id\n",
    "            annotation['i_id'] = image_id\n",
    "            annotation['kpts_2d'] = map(int,keypoints_2d)\n",
    "            annotation['kpts_3d'] = map(int,keypoints_3d)\n",
    "            human36m['annotations'].append(annotation)\n",
    "\n",
    "            image = {}\n",
    "            image['id']       = image_id\n",
    "            image['c_id']     = camera_id\n",
    "            image['s_id']     = subject_id\n",
    "            image['filename'] = 'human36m_train_%010d.jpg'%(image_id)\n",
    "            image['width']    = IMAGES_WIDTH\n",
    "            image['height']   = IMAGES_HEIGHT\n",
    "            image['video']    = video_filename\n",
    "            image['frame']    = frame_num\n",
    "            human36m['images'].append(image)\n",
    "\n",
    "            # save the image\n",
    "            image_filename = '%s/human36m_train_%010d.jpg'%(IMAGES_DIR,image_id)\n",
    "            imageio.imwrite(image_filename, crop)\n",
    "\n",
    "            if pad_left + pad_top != 0:\n",
    "                PADDED.append(image_id)\n",
    "            frame_num += SKIP_FRAMES\n",
    "\n",
    "        # close the frame reader before processing next action\n",
    "        frames.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<open file '/Users/Robert/Documents/Caltech/CS81_Depth_Research/datasets/human36m_original/Release-v1.1/H36M/rotationmatrices.csv', mode 'r' at 0x110865d20>\n"
     ]
    }
   ],
   "source": [
    "from postprocess_original_utils import load_rotation_matrices\n",
    "\n",
    "load_rotation_matrices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.91536173,  0.92816834, -0.91415495,  0.91415624, -0.90420742,\n",
       "          0.9222116 , -0.92582886,  0.92228155, -0.91495033,  0.91973647,\n",
       "         -0.9165777 ,  0.91829506, -0.90557642,  0.92126408, -0.92450697,\n",
       "          0.9228354 , -0.91156947,  0.93101628, -0.92090758,  0.92766705,\n",
       "         -0.90334862,  0.93157205, -0.92693442,  0.91546071, -0.9059013 ,\n",
       "          0.92166465, -0.90635406,  0.91754082],\n",
       "        [ 0.40180837,  0.37215384, -0.40277802, -0.40060706,  0.42657831,\n",
       "          0.38649076, -0.37286741, -0.37726887,  0.40348643,  0.39209902,\n",
       "         -0.39393484, -0.3850769 ,  0.42392654,  0.38860118, -0.37555597,\n",
       "         -0.37440015,  0.41064943,  0.36476269, -0.38473552, -0.36360628,\n",
       "          0.42691198,  0.36348288, -0.37323035, -0.39734607,  0.42171441,\n",
       "          0.38798487, -0.42053102, -0.39226322],\n",
       "        [ 0.02574754,  0.00224838, -0.04572295,  0.06190599,  0.02097347,\n",
       "          0.01227429, -0.06173178,  0.08405321,  0.00803635,  0.01852537,\n",
       "         -0.06856141,  0.09192373,  0.01475238,  0.01617474, -0.06515035,\n",
       "          0.09055029,  0.02020282,  0.01252435, -0.06251254,  0.08499598,\n",
       "          0.04132109, -0.00732918, -0.03862235,  0.0636223 ,  0.03872711,\n",
       "         -0.00141729, -0.04093881,  0.06517976]],\n",
       "\n",
       "       [[ 0.05154812,  0.08166409, -0.04562341, -0.05641001,  0.06390494,\n",
       "          0.09333184, -0.02357811, -0.02117765,  0.07174776,  0.10147807,\n",
       "         -0.01984532, -0.01553499,  0.06862813,  0.09922278, -0.01895501,\n",
       "         -0.01498208,  0.06090775,  0.08939715, -0.02568138, -0.01666269,\n",
       "          0.04153061,  0.06810069, -0.04725991, -0.04940628,  0.04449318,\n",
       "          0.07721055, -0.06032122, -0.04531905],\n",
       "        [ 0.18037357, -0.1977723 ,  0.2143085 , -0.2769532 ,  0.18368565,\n",
       "         -0.19167234,  0.22000056, -0.26645871,  0.1822276 , -0.1919146 ,\n",
       "          0.2160707 , -0.26706146,  0.18074372, -0.19461154,  0.21601111,\n",
       "         -0.26978659,  0.18347366, -0.19463753,  0.21992027, -0.26770413,\n",
       "          0.18295114, -0.19426748,  0.21824049, -0.26789168,  0.18571991,\n",
       "         -0.1869924 ,  0.22468715, -0.26600517],\n",
       "        [-0.98224649, -0.97684036, -0.97569994, -0.95922617, -0.98090557,\n",
       "         -0.9770112 , -0.97521476, -0.96361365, -0.98063518, -0.97615111,\n",
       "         -0.97617602, -0.96355427, -0.98113296, -0.97584896, -0.9762069 ,\n",
       "         -0.96280358, -0.9811359 , -0.97679291, -0.97517975, -0.96335707,\n",
       "         -0.98224441, -0.97858185, -0.97475001, -0.96218141, -0.98159486,\n",
       "         -0.97932241, -0.9725621 , -0.96290572]],\n",
       "\n",
       "       [[-0.39931903, -0.36309022,  0.40278931,  0.40141783, -0.42228557,\n",
       "         -0.37525316,  0.37720683,  0.38593814, -0.39713744, -0.3791926 ,\n",
       "          0.39936385,  0.39559178, -0.4185947 , -0.37606827,  0.38069353,\n",
       "          0.38490306, -0.40660958, -0.3538599 ,  0.38893406,  0.37303645,\n",
       "         -0.42689165, -0.35712157,  0.37223525,  0.39936288, -0.42114509,\n",
       "         -0.3802273 ,  0.41819095,  0.39505065],\n",
       "        [-0.89778361,  0.90685591, -0.88985489,  0.87339047, -0.88560179,\n",
       "          0.90215664, -0.90142645,  0.88694304, -0.8966559 ,  0.89968169,\n",
       "         -0.89338054,  0.88339909, -0.88747845,  0.90061949, -0.90127516,\n",
       "          0.88715259, -0.89314302,  0.91052974, -0.89644501,  0.89225836,\n",
       "         -0.88559305,  0.91112038, -0.90170405,  0.87769594, -0.88750497,\n",
       "          0.90249741, -0.87901612,  0.88055143],\n",
       "        [-0.18581953, -0.21395759, -0.21428728, -0.27577674, -0.19335039,\n",
       "         -0.21283435, -0.21247438, -0.25373962, -0.19567845, -0.21630031,\n",
       "         -0.20586335, -0.25122339, -0.19277053, -0.21784671, -0.20682245,\n",
       "         -0.25457791, -0.19226072, -0.21381946, -0.21240678, -0.25439896,\n",
       "         -0.18299858, -0.20572759, -0.21993346, -0.2648757 , -0.18700732,\n",
       "         -0.20230081, -0.22901305, -0.2618476 ]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy.io as sio\n",
    "ROTATION_MATRICES_PATH = \"/Users/Robert/Documents/Caltech/CS81_Depth_Research/\" \\\n",
    "                         \"datasets/human36m_original/Release-v1.1/H36M/\" \\\n",
    "                         \"rotationmatrices.mat\"\n",
    "sio.loadmat(ROTATION_MATRICES_PATH)['cam_info']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read in 1000 HIT total assignments\n",
      "979 good, 21 flagged, 0 error, 0 rejected\n",
      "105812 images in human3.6 dataset\n",
      "979 annotations matched with ground truth\n",
      "Output data to /Users/Robert/Documents/Caltech/CS81_Depth_Research/results/human36m_processed_data.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from postprocess_original_utils import (load_data, wrongness,\n",
    "    get_keypoint_comparison_depths, metaperson_comparisons, worker_comparisons,\n",
    "    fraction_of_correct_comparisons_per_trial)\n",
    "\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "import itertools\n",
    "import copy\n",
    "import difflib\n",
    "import random\n",
    "from collections import Counter, defaultdict\n",
    "from lean_correction import correct_lean\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from constants import (HUMAN_ANNOTATION_PATH, COCO_ANNOTATION_PATH,\n",
    "    HUMAN_RAW_RESULT_PATH, COCO_RAW_RESULT_PATH, KEYPTS_RELATIVE_DEPTH_PATH,\n",
    "    KEYCMPS_RESULT_PATH, HUMAN_OUTPUT_PATH)\n",
    "\n",
    "with open(HUMAN_ANNOTATION_PATH) as f:\n",
    "        _human_dataset = json.load(f)\n",
    "        correct_lean(_human_dataset)\n",
    "\n",
    "data = load_data(load_from_file=False, full_ordering=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "subj_id_to_yvals = {}\n",
    "axis_to_sort = 1\n",
    "for d in _human_dataset['annotations']:\n",
    "    subj_id_to_yvals[d['i_id']] = list(d['kpts_3d'])\n",
    "#     subj_id_to_yvals[d['i_id']][3:6] = [] # Remove neck keypoint\n",
    "    subj_id_to_yvals[d['i_id']] = subj_id_to_yvals[d['i_id']][axis_to_sort::3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-74031c284cc0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mkpt1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkpt2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_k\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_k\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcomp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mkpt1_depth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myvals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkpt1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0mkpt2_depth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myvals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkpt2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m             \u001b[0mdepth_diff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkpt1_depth\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mkpt2_depth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mtrial_comp_res\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdepth_diff\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomp_res\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# Fancy hybrid per trial and per assignment\n",
    "\n",
    "fractions_of_correct_comps_per_trial = []\n",
    "fractions_of_correct_comps_per_assignment = []\n",
    "for d in data:\n",
    "    assignment_comp_res = []\n",
    "    for t in d['trials']:\n",
    "        subj_id = t['human_subj_id']\n",
    "        yvals = subj_id_to_yvals[subj_id]\n",
    "        trial_comp_res = []\n",
    "        for comp, comp_res in t['depth']['keypoint_comparisons_res'].iteritems():\n",
    "            kpt1, kpt2 = [int(_k) for _k in comp.split(',')]\n",
    "            kpt1_depth = yvals[kpt1]\n",
    "            kpt2_depth = yvals[kpt2]\n",
    "            depth_diff = kpt1_depth - kpt2_depth\n",
    "            trial_comp_res.append(int(np.sign(depth_diff) == np.sign(comp_res)))\n",
    "        assignment_comp_res.extend(trial_comp_res)\n",
    "        fractions_of_correct_comps_per_trial.append(sum(trial_comp_res) * 1.0 / len(trial_comp_res))\n",
    "    fractions_of_correct_comps_per_assignment.append(sum(assignment_comp_res) * 1.0 / len(assignment_comp_res))\n",
    "    \n",
    "# fractions_of_correct_comps_per_trial\n",
    "# fractions_of_correct_comps_per_assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-56356602357a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mkpt1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkpt2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_k\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_k\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcomp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mkpt1_depth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myvals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkpt1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0mkpt2_depth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myvals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkpt2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0mdepth_diff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkpt1_depth\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mkpt2_depth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0massignment_comp_res\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdepth_diff\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomp_res\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "fractions_of_correct_comps_per_assignment = []\n",
    "\n",
    "for d in data:\n",
    "    assignment_comp_res = []\n",
    "    for t in d['trials']:\n",
    "        subj_id = t['human_subj_id']\n",
    "        yvals = subj_id_to_yvals[subj_id]\n",
    "        for comp, comp_res in t['depth']['keypoint_comparisons_res'].iteritems():\n",
    "            kpt1, kpt2 = [int(_k) for _k in comp.split(',')]\n",
    "            kpt1_depth = yvals[kpt1]\n",
    "            kpt2_depth = yvals[kpt2]\n",
    "            depth_diff = kpt1_depth - kpt2_depth\n",
    "            assignment_comp_res.append(int(np.sign(depth_diff) == np.sign(comp_res)))\n",
    "    fractions_of_correct_comps_per_assignment.append(sum(assignment_comp_res) * 1.0 / len(assignment_comp_res))\n",
    "\n",
    "# fractions_of_correct_comps_per_assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "subj_id_to_trials = {}\n",
    "\n",
    "for d in data:\n",
    "    for t in d['trials']:\n",
    "        subj_id = t['human_subj_id']\n",
    "        if subj_id in subj_id_to_trials:\n",
    "            subj_id_to_trials[subj_id].append(t['depth']['keypoint_comparisons_res'])\n",
    "        else:\n",
    "            subj_id_to_trials[subj_id] = [t['depth']['keypoint_comparisons_res']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fractions_of_correct_comps_per_subject = []\n",
    "\n",
    "for subj_id in subj_id_to_trials.keys():\n",
    "    subj_comp_res = []\n",
    "    trials = subj_id_to_trials[subj_id]\n",
    "    yvals = subj_id_to_yvals[subj_id]\n",
    "    for t in trials:\n",
    "        for comp, comp_res in t.iteritems():\n",
    "            kpt1, kpt2 = [int(_k) for _k in comp.split(',')]\n",
    "            kpt1_depth = yvals[kpt1]\n",
    "            kpt2_depth = yvals[kpt2]\n",
    "            depth_diff = kpt1_depth - kpt2_depth\n",
    "            subj_comp_res.append(int(np.sign(depth_diff) == np.sign(comp_res)))\n",
    "    fractions_of_correct_comps_per_subject.append(sum(subj_comp_res) * 1.0 / len(subj_comp_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hit_id_to_trials = {}\n",
    "\n",
    "for d in data:\n",
    "    hit_id = d['hit_id']\n",
    "    if hit_id in hit_id_to_trials:\n",
    "        hit_id_to_trials[hit_id].append(d['trials'])\n",
    "    else:\n",
    "        hit_id_to_trials[hit_id] = [d['trials']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fractions_of_correct_comps_per_hit = []\n",
    "\n",
    "for hit_id in hit_id_to_trials.keys():\n",
    "    hit_comp_res = []\n",
    "    trials = hit_id_to_trials[hit_id]\n",
    "    for trial in trials:\n",
    "        for t in trial:\n",
    "            subj_id = t['human_subj_id']\n",
    "            yvals = subj_id_to_yvals[subj_id]\n",
    "            for comp, comp_res in t['depth']['keypoint_comparisons_res'].iteritems():\n",
    "                kpt1, kpt2 = [int(_k) for _k in comp.split(',')]\n",
    "                kpt1_depth = yvals[kpt1]\n",
    "                kpt2_depth = yvals[kpt2]\n",
    "                depth_diff = kpt1_depth - kpt2_depth\n",
    "                hit_comp_res.append(int(np.sign(depth_diff) == np.sign(comp_res)))\n",
    "    fractions_of_correct_comps_per_hit.append(sum(hit_comp_res) * 1.0 / len(hit_comp_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "worker_id_to_trials = {}\n",
    "\n",
    "for d in data:\n",
    "    worker_id = d['worker_id']\n",
    "    if worker_id in worker_id_to_trials:\n",
    "        worker_id_to_trials[worker_id].append(d['trials'])\n",
    "    else:\n",
    "        worker_id_to_trials[worker_id] = [d['trials']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fractions_of_correct_comps_per_worker = []\n",
    "\n",
    "for worker_id in worker_id_to_trials.keys():\n",
    "    worker_comp_res = []\n",
    "    trials = worker_id_to_trials[worker_id]\n",
    "    for trial in trials:\n",
    "        for t in trial:\n",
    "            subj_id = t['human_subj_id']\n",
    "            yvals = subj_id_to_yvals[subj_id]\n",
    "            for comp, comp_res in t['depth']['keypoint_comparisons_res'].iteritems():\n",
    "                kpt1, kpt2 = [int(_k) for _k in comp.split(',')]\n",
    "                kpt1_depth = yvals[kpt1]\n",
    "                kpt2_depth = yvals[kpt2]\n",
    "                depth_diff = kpt1_depth - kpt2_depth\n",
    "                worker_comp_res.append(int(np.sign(depth_diff) == np.sign(comp_res)))\n",
    "    fractions_of_correct_comps_per_worker.append(sum(worker_comp_res) * 1.0 / len(worker_comp_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "subj_id_to_metatrials = {}\n",
    "\n",
    "for subj_id in subj_id_to_trials.keys():\n",
    "    metatrial = {}\n",
    "    for t in subj_id_to_trials[subj_id]:\n",
    "        for comp, comp_res in t.iteritems():\n",
    "            if comp in metatrial:\n",
    "                metatrial[comp] += comp_res\n",
    "            else:\n",
    "                metatrial[comp] = comp_res\n",
    "    for comp, comp_res in metatrial.iteritems():\n",
    "        metatrial[comp] = np.sign(comp_res) if comp_res != 0 else random.choice([-1, 1])\n",
    "    subj_id_to_metatrials[subj_id] = metatrial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fractions_of_correct_comps_per_metatrial = []\n",
    "\n",
    "for subj_id, t in subj_id_to_metatrials.iteritems():\n",
    "    yvals = subj_id_to_yvals[subj_id]\n",
    "    trial_comp_res = []\n",
    "    for comp, comp_res in t.iteritems():\n",
    "        kpt1, kpt2 = [int(_k) for _k in comp.split(',')]\n",
    "        kpt1_depth = yvals[kpt1]\n",
    "        kpt2_depth = yvals[kpt2]\n",
    "        depth_diff = kpt1_depth - kpt2_depth\n",
    "        trial_comp_res.append(int(np.sign(depth_diff) == np.sign(comp_res)))\n",
    "    fractions_of_correct_comps_per_metatrial.append(sum(trial_comp_res) * 1.0 / len(trial_comp_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hit_id_to_subj_ids = {}\n",
    "\n",
    "for d in data:\n",
    "    hit_id_to_subj_ids[d['hit_id']] = d['human_subj_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fractions_of_correct_comps_per_metahit = []\n",
    "\n",
    "for hit_id, subj_ids in hit_id_to_subj_ids.iteritems():\n",
    "    metahit_comp_res = []\n",
    "    for subj_id in subj_ids:\n",
    "        t = subj_id_to_metatrials[subj_id]\n",
    "        yvals = subj_id_to_yvals[subj_id]\n",
    "        for comp, comp_res in t.iteritems():\n",
    "            kpt1, kpt2 = [int(_k) for _k in comp.split(',')]\n",
    "            kpt1_depth = yvals[kpt1]\n",
    "            kpt2_depth = yvals[kpt2]\n",
    "            depth_diff = kpt1_depth - kpt2_depth\n",
    "            metahit_comp_res.append(int(np.sign(depth_diff) == np.sign(comp_res)))\n",
    "    fractions_of_correct_comps_per_metahit.append(sum(metahit_comp_res) * 1.0 / len(metahit_comp_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "workers_ranked_worst_to_best = []\n",
    "\n",
    "worker_accuracy = {}\n",
    "for worker_id in worker_id_to_trials.keys():\n",
    "    worker_comp_res = []\n",
    "    trials = worker_id_to_trials[worker_id]\n",
    "    for trial in trials:\n",
    "        for t in trial:\n",
    "            subj_id = t['human_subj_id']\n",
    "            yvals = subj_id_to_yvals[subj_id]\n",
    "            for comp, comp_res in t['depth']['keypoint_comparisons_res'].iteritems():\n",
    "                kpt1, kpt2 = [int(_k) for _k in comp.split(',')]\n",
    "                kpt1_depth = yvals[kpt1]\n",
    "                kpt2_depth = yvals[kpt2]\n",
    "                depth_diff = kpt1_depth - kpt2_depth\n",
    "                worker_comp_res.append(int(np.sign(depth_diff) == np.sign(comp_res)))\n",
    "    worker_accuracy[worker_id] = sum(worker_comp_res) * 1.0 / len(worker_comp_res)\n",
    "workers_ranked_worst_to_best = [key for key, value in sorted(worker_accuracy.iteritems(), key=lambda (k,v): (v,k))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "distance_of_wrong_comps = []\n",
    "distance_of_correct_comps = []\n",
    "\n",
    "FRACTION_OF_WORKERS_TO_EXCLUDE = 0.1\n",
    "accepted_worker_ids = workers_ranked_worst_to_best[int(len(workers_ranked_worst_to_best) * FRACTION_OF_WORKERS_TO_EXCLUDE):]\n",
    "\n",
    "for d in data:\n",
    "    if d['worker_id'] not in accepted_worker_ids:\n",
    "        continue\n",
    "    for t in d['trials']:\n",
    "        subj_id = t['human_subj_id']\n",
    "        yvals = subj_id_to_yvals[subj_id]\n",
    "        for comp, comp_res in t['depth']['keypoint_comparisons_res'].iteritems():\n",
    "            kpt1, kpt2 = [int(_k) for _k in comp.split(',')]\n",
    "            kpt1_depth = yvals[kpt1]\n",
    "            kpt2_depth = yvals[kpt2]\n",
    "            depth_diff = kpt1_depth - kpt2_depth\n",
    "            if np.sign(depth_diff) == np.sign(comp_res):\n",
    "                distance_of_correct_comps.append(abs(depth_diff))\n",
    "            else:\n",
    "                distance_of_wrong_comps.append(abs(depth_diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "distance_of_wrong_metacomps = []\n",
    "distance_of_correct_metacomps = []\n",
    "\n",
    "for subj_id, t in subj_id_to_metatrials.iteritems():\n",
    "    yvals = subj_id_to_yvals[subj_id]\n",
    "    for comp, comp_res in t.iteritems():\n",
    "        kpt1, kpt2 = [int(_k) for _k in comp.split(',')]\n",
    "        kpt1_depth = yvals[kpt1]\n",
    "        kpt2_depth = yvals[kpt2]\n",
    "        depth_diff = kpt1_depth - kpt2_depth\n",
    "        if np.sign(depth_diff) == np.sign(comp_res):\n",
    "            distance_of_correct_metacomps.append(abs(depth_diff))\n",
    "        else:\n",
    "            distance_of_wrong_metacomps.append(abs(depth_diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "subj_id_accuracy = {}\n",
    "subjects_ranked_hard_to_easy = []\n",
    "\n",
    "for subj_id in subj_id_to_trials.keys():\n",
    "    subj_comp_res = []\n",
    "    trials = subj_id_to_trials[subj_id]\n",
    "    yvals = subj_id_to_yvals[subj_id]\n",
    "    for t in trials:\n",
    "        for comp, comp_res in t.iteritems():\n",
    "            kpt1, kpt2 = [int(_k) for _k in comp.split(',')]\n",
    "            kpt1_depth = yvals[kpt1]\n",
    "            kpt2_depth = yvals[kpt2]\n",
    "            depth_diff = kpt1_depth - kpt2_depth\n",
    "            subj_comp_res.append(int(np.sign(depth_diff) == np.sign(comp_res)))\n",
    "    subj_id_accuracy[subj_id] = sum(subj_comp_res) * 1.0 / len(subj_comp_res)\n",
    "subjects_ranked_hard_to_easy = [key for key, value in sorted(subj_id_accuracy.iteritems(), key=lambda (k,v): (v,k))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "subj_id_to_image_path = {}\n",
    "\n",
    "for d in _human_dataset['images']:\n",
    "    subj_id_to_image_path[d['id']] = d['filename']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(_human_dataset['annotations'][0]['kpts_3d']) / 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "{u'_keypoints_bbox': [0, 0, 256, 256], u'_image_keypoints': [110, 21, 2, 137, 43, 2, 86, 58, 2, 151, 39, 2, 89, 94, 2, 146, 45, 2, 114, 88, 2, 127, 117, 2, 87, 118, 2, 169, 163, 2, 119, 162, 2, 142, 214, 2, 114, 219, 2], u'_comps': [[0, 2], [6, 11], [5, 6], [2, 3], [5, 8]], u'_human_img_src': u'/static/images/human/human36m_train_0000279782.jpg', u'_id': ObjectId('5accb516494f991b4a06bfb0'), u'_human_subj_id': 279782, u'_human_img_id': 279782}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
